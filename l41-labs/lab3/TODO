= State machine behaviour use -B -q
= Plot two directions of TCP state machine
  = client vs. server
  = TCP header flags (or syscall/timer) of received packet cause the transition
  = Instrument headers in syncache
  + Compare to RFC 793
= Affect of latency on state machine (0-40ms)
= Probe effect
= ipc/ipc-static -B -q -i tcp 2thread

= Only I/O loop analysis
= ipc/ipc-static -i tcp -b 1048576 (-s) 2thread
= Flush cache
= Bandwidth vs. Latency [0-40ms] (with and without resizing)
= Bandwidth vs. Real-time (with and without resizing)
  = Stack congestion and last received windows graph
  - find point at which slow-start occurs
= Only consider tcp steady state (i.e. no handshake or connection close)
= Sequence Number vs. Real-time (with and without resizing)
  = Few packets past slow start
  = 10ms and 20ms RTT (i.e. 5 and 10ms latency)
  - Effect of latency on time to enter steady state
    - Then effect on bandwidth
  x graph bytes-in-flight

= Probe effect: DUMMYNET, DTrace
= Statistical analysis

*: denotes completed tasks
~: double check with lecturer
+: by hand
=: already in report
x: wont do
#####

FreBSD maximum segment lifetime (MSL): 30 seconds
sysctl net.inet.ip | grep forward
sysctl net.isr.direct
net.inet.tcp.recvspace: 65536
net.inet.tcp.recvbuf_max: 2097152
net.inet.tcp.recvbuf_auto: 1
net.inet.tcp.sendspace: 32768
net.inet.tcp.sendbuf_max: 2097152
net.inet.tcp.sendbuf_auto: 1
net.inet.tcp.initcwnd_segments: 10
net.inet.tcp.drop_synfin: 0
net.inet.tcp.ecn.enable: 2 (described in tcp(4) man page)
net.inet.tcp.minmss: 216
net.inet.sctp.initial_cwnd: 3
net.inet.tcp.delayed_ack: 1

sysctla -a | grep rfc

rfc3465-> ABC enabled
sysctl -a | grep net.inet.tcp.cc

disabling Nagle's algorithm not effective (TCP_NODELAY)
########
send socket buffer resizing:
tcp_sndbuf_autoscale: tcp_output.c

slow-start initial config:
cc_conn_init

int tcp_compute_initwnd (in tcp_input.c) initial cwnd size is calculated using following codepath:
	if (V_tcp_initcwnd_segments)
		return min(V_tcp_initcwnd_segments * maxseg,
			max(2 * maxseg, V_tcp_initcwnd_segments * 1460));

Where is cwnd set?:
	LINE 2470:
		TF_SACK_PERMIT enabled
			FAST_RECOVERY disabled
			LINE 2605: rfc3042 enabled
				LINE 2629+ cwnd increased by to account for dupacks
https://github.com/freebsd/freebsd/blob/master/sys/netinet/tcp_var.h
https://github.com/freebsd/freebsd/blob/master/sys/netinet/tcp_output.c
https://github.com/freebsd/freebsd/blob/master/sys/netinet/tcp_input.c
http://www.ico.aha.ru/h/The_Design_and_Implementation_of_the_FreeBSD_Operating_System/ch13lev1sec7.htm


INSTRUMENT: tcp_xmit_timer, where is srtt used and calculated, how often is tcp_input/tcp_output called with different latencies

#####

Sequence diagram:

Intuitively the sequence numbers and acknowledgements should at every time-step increase linearly throughout the lifetime of the connection. Additionally we expect to see a delay between the transmission of a TCP segment and the acknowledgement by the receiver. In the case 5 ms latency (10 ms RTT) we detect two differences to the ideal behaviour. On FIGURE we see the time for TCP to reach the linear congestion window increase of congestion avoidance is minimal and occurs in a fraction of a nanosecond. On our plot of this connection's receive and congestion window updates as seen by the sender we see that the exponential increase of cwnd performed by slow-start is barely visible at our granularity of measurement. Furthermore, many of the sequence numbers are acknowledged at the same timestamp. At low latency the receive window continuously emptied across the whole lifetime of the connection indicated by the fluctation of the wnd size between SIZE and SIZE. (INSTRUMENT TCP INPUT/OUTPUT pattern)

Throughput vs. Latency:
The congestion window is resized based upon the current phase of congestion control, the congestion control algorithm used and metrics collected by tcp throughout the connection lifetime. During slow-start the congestion window is increased exponentially until a timeout occurs but not beyond the advertised receive window size and the slow-start threshold (snd\_ssthresh). Once snd\_ssthresh is reached or a timeout occurred the congestion window gradually increases. The actual window updates are performed within tcp\_input.

Although our benchmarks inject a DUMMYNET latency of 0-40 ms


Future work:
better granularity of latency injection and measurement
